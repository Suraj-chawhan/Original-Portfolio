"use client";

import React, { useRef, useState, useEffect } from "react";
import { Canvas, useFrame } from "@react-three/fiber";
import { useGLTF } from "@react-three/drei";
import gsap from "gsap";

import SkillSection from "../../Components/Skills";
import RotatingStars from "../../Components/RotatingStar";
import GameDesk from "../../Components/GameDesk";
import Navbar from "../../Components/Navbar";
import Projects from "../../Components/ProjectSection";
import Contact from "../../Components/Contact";

// âœ… Fireball model preload
function useFireballPreload() {
Â  useEffect(() => {
Â  Â  useGLTF.preload("/fireball_vfx.glb");
Â  }, []);
}

function Fireball({ isSpeaking }) {
Â  useFireballPreload();
Â  const { scene } = useGLTF("/fireball_vfx.glb");
Â  const ref = useRef();
Â  const directionRef = useRef(1);

Â  const baseScale = 1;
Â  const maxScale = 1.5;
Â  const pulseSpeed = 0.01;

Â  useFrame(() => {
Â  Â  if (!ref.current) return;
Â  Â  ref.current.rotation.y += 0.01;

Â  Â  if (isSpeaking) {
Â  Â  Â  const currentScale = ref.current.scale.x;
Â  Â  Â  if (currentScale >= maxScale || currentScale <= baseScale) {
Â  Â  Â  Â  directionRef.current *= -1;
Â  Â  Â  }
Â  Â  Â  const scaleChange = directionRef.current * pulseSpeed;
Â  Â  Â  const newScale = currentScale + scaleChange;
Â  Â  Â  ref.current.scale.set(newScale, newScale, newScale);
Â  Â  Â  ref.current.position.y = Math.sin(Date.now() * 0.005) * 0.1;
Â  Â  } else {
Â  Â  Â  ref.current.scale.set(baseScale, baseScale, baseScale);
Â  Â  Â  ref.current.position.y = 0;
Â  Â  }
Â  });

Â  return (
Â  Â  <primitive
Â  Â  Â  object={scene}
Â  Â  Â  ref={ref}
Â  Â  Â  position={[0.1, 0, 0.2]}
Â  Â  Â  rotation={[1, 0, 0]}
Â  Â  />
Â  );
}

// âœ… WebGL support check
function isWebGLSupported() {
Â  if (typeof window === "undefined") return false;
Â  try {
Â  Â  const canvas = document.createElement("canvas");
Â  Â  return !!(
Â  Â  Â  window.WebGLRenderingContext &&
Â  Â  Â  (canvas.getContext("webgl") || canvas.getContext("experimental-webgl"))
Â  Â  );
Â  } catch (e) {
Â  Â  return false;
Â  }
}

export default function CharacterCanvas() {
Â  const audioRef = useRef(null);
Â  const [isListening, setIsListening] = useState(false);
Â  const [isSpeaking, setIsSpeaking] = useState(false);
Â  const headingRef = useRef();
Â  const [webgl, setWebgl] = useState(true);

Â  useEffect(() => {
Â  Â  setWebgl(isWebGLSupported());

Â  Â  if (headingRef.current?.children) {
Â  Â  Â  gsap.fromTo(
Â  Â  Â  Â  headingRef.current.children,
Â  Â  Â  Â  { y: 50, opacity: 0 },
Â  Â  Â  Â  {
Â  Â  Â  Â  Â  y: 0,
Â  Â  Â  Â  Â  opacity: 1,
Â  Â  Â  Â  Â  duration: 1,
Â  Â  Â  Â  Â  stagger: 0.2,
Â  Â  Â  Â  Â  ease: "power3.out",
Â  Â  Â  Â  Â  delay: 0.5,
Â  Â  Â  Â  }
Â  Â  Â  );
Â  Â  }
Â  }, []);

Â  const handleSTT = () => {
Â  Â  if (typeof window === "undefined") return;

Â  Â  const SpeechRecognition =
Â  Â  Â  window.SpeechRecognition || window.webkitSpeechRecognition;
Â  Â  if (!SpeechRecognition) return alert("SpeechRecognition not supported");

Â  Â  const recognition = new SpeechRecognition();
Â  Â  recognition.lang = "en-US";
Â  Â  recognition.interimResults = false;
Â  Â  recognition.maxAlternatives = 1;

Â  Â  recognition.onstart = () => setIsListening(true);
Â  Â  recognition.onend = () => setIsListening(false);

Â  Â  recognition.onresult = async (e) => {
Â  Â  Â  const text = e.results[0][0].transcript;
Â  Â  Â  console.log("User said:", text);

Â  Â  Â  try {
Â  Â  Â  Â  const res1 = await fetch(`${process.env.NEXT_PUBLIC_NODE_URL}/chat`, {
Â  Â  Â  Â  Â  method: "POST",
Â  Â  Â  Â  Â  headers: { "Content-Type": "application/json" },
Â  Â  Â  Â  Â  body: JSON.stringify({ prompt: text }),
Â  Â  Â  Â  });

Â  Â  Â  Â  const responseText = await res1.text();

Â  Â  Â  Â  const res2 = await fetch(`${process.env.NEXT_PUBLIC_PYTHON_URL}/ask`, {
Â  Â  Â  Â  Â  method: "POST",
Â  Â  Â  Â  Â  headers: { "Content-Type": "application/json" },
Â  Â  Â  Â  Â  body: JSON.stringify({ prompt: responseText }),
Â  Â  Â  Â  });

Â  Â  Â  Â  const blob = await res2.blob();
Â  Â  Â  Â  const audioUrl = URL.createObjectURL(blob);

Â  Â  Â  Â  const audio = audioRef.current;
Â  Â  Â  Â  audio.src = audioUrl;
Â  Â  Â  Â  audio.play();
Â  Â  Â  Â  setIsSpeaking(true);
Â  Â  Â  } catch (err) {
Â  Â  Â  Â  console.error("API Error:", err);
Â  Â  Â  }
Â  Â  };

Â  Â  recognition.onerror = (err) => {
Â  Â  Â  console.error("STT Error:", err);
Â  Â  Â  setIsListening(false);
Â  Â  };

Â  Â  recognition.start();
Â  };

Â  return (
Â  Â  <div style={{ fontFamily: "sans-serif", backgroundColor: "#000", color: "#fff", minHeight: "100vh" }}>
Â  Â  Â  <Navbar />

Â  Â  Â  <section id="hero" style={{ position: "relative", height: "70vh", display: "flex", flexDirection: "column", alignItems: "center", justifyContent: "center", overflow: "hidden" }}>
Â  Â  Â  Â  {webgl ? (
Â  Â  Â  Â  Â  <Canvas style={{ position: "absolute", top: 0, left: 0, width: "100%", height: "100%" }}>
Â  Â  Â  Â  Â  Â  <RotatingStars />
Â  Â  Â  Â  Â  Â  <GameDesk />
Â  Â  Â  Â  Â  Â  <ambientLight intensity={2} />
Â  Â  Â  Â  Â  Â  <spotLight position={[-5, 5, 5]} angle={0.4} intensity={3} color="#6A0DAD" castShadow />
Â  Â  Â  Â  Â  Â  <spotLight position={[5, 5, 5]} angle={0.4} intensity={3} color="#00FFFF" castShadow />
Â  Â  Â  Â  Â  </Canvas>
Â  Â  Â  Â  ) : (
Â  Â  Â  Â  Â  <div style={{ position: "absolute", top: 0, left: 0, width: "100%", height: "100%", background: "#111", display: "flex", alignItems: "center", justifyContent: "center", color: "#ccc" }}>
Â  Â  Â  Â  Â  Â  <p>âš ï¸ Your browser does not support WebGL. 3D content is not available.</p>
Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  )}

Â  Â  Â  Â  <div style={{ position: "relative", zIndex: 1, textAlign: "center", color: "#fff", padding: "2rem" }} ref={headingRef}>
Â  Â  Â  Â  Â  <h1 className="passAnimation" style={{ fontSize: "3rem", marginBottom: "1rem" }}>
Â  Â  Â  Â  Â  Â  Hi, I&apos;m Suraj Chawhan
Â  Â  Â  Â  Â  </h1>
Â  Â  Â  Â  Â  <p style={{ fontSize: "1.25rem", maxWidth: "600px", margin: "0 auto" }}>
Â  Â  Â  Â  Â  Â  Iâ€™m a passionate developer creating innovative AI and interactive 3D experiences.
Â  Â  Â  Â  Â  </p>
Â  Â  Â  Â  </div>
Â  Â  Â  </section>

Â  Â  Â  <SkillSection />
Â  Â  Â  <Projects />

Â  Â  Â  <section id="ai" style={{ padding: "40px 20px", textAlign: "center" }}>
Â  Â  Â  Â  <h2 style={{ fontSize: "2.5rem", color: "white", marginBottom: "10px" }}>
Â  Â  Â  Â  Â  Talk to my AI Assistant
Â  Â  Â  Â  </h2>

Â  Â  Â  Â  {webgl ? (
Â  Â  Â  Â  Â  <Canvas style={{ height: "60vh", borderRadius: "12px" }} camera={{ position: [0, 2, 5] }}>
Â  Â  Â  Â  Â  Â  <ambientLight intensity={4} />
Â  Â  Â  Â  Â  Â  <spotLight angle={0.4} intensity={60} color="purple" position={[-2, 5, 5]} />
Â  Â  Â  Â  Â  Â  <spotLight angle={0.4} intensity={60} color="blue" position={[2, 5, 5]} />
Â  Â  Â  Â  Â  Â  <directionalLight position={[2, 4, 2]} intensity={5} />
Â  Â  Â  Â  Â  Â  <RotatingStars />
Â  Â  Â  Â  Â  Â  <Fireball isSpeaking={isSpeaking} />
Â  Â  Â  Â  Â  </Canvas>
Â  Â  Â  Â  ) : (
Â  Â  Â  Â  Â  <div style={{ height: "200px", padding: "20px", background: "#111", borderRadius: "12px", color: "#ccc", display: "flex", alignItems: "center", justifyContent: "center" }}>
Â  Â  Â  Â  Â  Â  <p>âš ï¸ WebGL not supported. Assistant 3D view disabled.</p>
Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  )}

Â  Â  Â  Â  <button
Â  Â  Â  Â  Â  onClick={handleSTT}
Â  Â  Â  Â  Â  style={{
Â  Â  Â  Â  Â  Â  marginTop: "25px",
Â  Â  Â  Â  Â  Â  padding: "12px 24px",
Â  Â  Â  Â  Â  Â  fontSize: "1rem",
Â  Â  Â  Â  Â  Â  fontWeight: "bold",
Â  Â  Â  Â  Â  Â  backgroundColor: "#6A0DAD",
Â  Â  Â  Â  Â  Â  color: "white",
Â  Â  Â  Â  Â  Â  border: "none",
Â  Â  Â  Â  Â  Â  borderRadius: "8px",
Â  Â  Â  Â  Â  Â  cursor: "pointer",
Â  Â  Â  Â  Â  Â  transition: "background-color 0.3s ease",
Â  Â  Â  Â  Â  }}
Â  Â  Â  Â  >
Â  Â  Â  Â  Â  ğŸ¤ {isListening ? "Listening..." : "Speak to AI"}
Â  Â  Â  Â  </button>

Â  Â  Â  Â  <audio ref={audioRef} crossOrigin="anonymous" onEnded={() => setIsSpeaking(false)} />
Â  Â  Â  </section>

Â  Â  Â  <Contact />
Â  Â  </div>
Â  );
      }
  
